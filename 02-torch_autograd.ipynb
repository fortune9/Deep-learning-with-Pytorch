{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5926ed9",
   "metadata": {},
   "source": [
    "# title: PyTorch Autograd\n",
    "\n",
    "torch.autograd is PyTorch’s automatic differentiation engine that powers neural network training. In this section, you will get a conceptual understanding of how autograd helps a neural network train.\n",
    "\n",
    "Neural networks (NNs) are a collection of nested functions that are executed on some input data. These functions are defined by parameters (consisting of weights and biases), which in PyTorch are stored in tensors.\n",
    "\n",
    "Training a NN happens in two steps:\n",
    "\n",
    "**Forward Propagation**: In forward prop, the NN makes its best guess about the correct output. It runs the input data through each of its functions to make this guess.\n",
    "\n",
    "**Backward Propagation**: In backprop, the NN adjusts its parameters proportionate to the error in its guess. It does this by traversing backwards from the output, collecting the derivatives of the error with respect to the parameters of the functions (gradients), and optimizing the parameters using gradient descent. For a more detailed walkthrough of backprop, check out this video from 3Blue1Brown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e85e2c",
   "metadata": {},
   "source": [
    "## Neural Networks demonstration in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "757a697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "# create a model\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "# generate a random data sensor to represent an image with 3 channels and 64x64 pixels\n",
    "data = torch.randn(1, 3, 64, 64)\n",
    "# and also its initial labels\n",
    "labels = torch.rand(1,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f039b80f",
   "metadata": {},
   "source": [
    "Next we run the input data through the model through each layer to make a prediction. And this is the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2004f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(data) # forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e03deb7",
   "metadata": {},
   "source": [
    "Next we will do the following:\n",
    "- compare the predictions and the corresponding labels to calculate error or loss\n",
    "- backpropagate the error through the network via .backward() method on the error tensor\n",
    "- torch.autograd calculates and stores the gradients for each model parameter in the .grad attribute of each parameter tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfc5b6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward() # backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7e8721e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x0000014C262AEB30>\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters()) # check the model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccdb3ca",
   "metadata": {},
   "source": [
    "Then we optimize the model using an optimizer SGD with learning rate 0.01 and momentum 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1113a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9a3dac",
   "metadata": {},
   "source": [
    "And finally we need to call .step to initiate gradient descent and the optimizer will adjust each parameter by its greadient stored in the .grad attribute of each parameter tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ec4486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step() #gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e771979",
   "metadata": {},
   "source": [
    "## Differentiation in Autograd\n",
    "\n",
    "Let’s take a look at how autograd collects gradients. We create two tensors a and b with requires_grad=True. This signals to autograd that every operation on them should be tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "028b7bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "# also create a new tensor from a and b\n",
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5557fc27",
   "metadata": {},
   "source": [
    "Assuming a and be to be the parameters of an NN, and Q to be the error, in NN training, we need gradients of the error regarding each parameter, i.e.:\n",
    "$$\n",
    "\\frac{\\partial Q}{\\partial a} = 9a^2\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial Q}{\\partial b} = -2b\n",
    "$$\n",
    "\n",
    "When we call .backward() on Q, autograd computes the gradients of Q with respect to a and b. The gradients are stored in the .grad attribute of each tensor. We can access them as follows:\n",
    "```python\n",
    "a.grad, b.grad\n",
    "```\n",
    "\n",
    "We need to explicitly pass a gradient argument to Q.backward() because it is a verctor with the same shape as Q, and its values are 1 because the gradients are for itself, i.e.:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Q}{\\partial Q} = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5fe5ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2019fe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "# check if collected gradients are correct\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed810feb",
   "metadata": {},
   "source": [
    "Mathematically, if a function $\\vec{y} = f(\\vec{x})$ is a vector function, then the gradient of $\\vec{y}$ with respect to $\\vec{x}$ is a Jacobian matrix $J$ defined as follows:\n",
    "$$\n",
    "J = \\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\\n",
    "\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\cdots & \\frac{\\partial y_2}{\\partial x_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\frac{\\partial y_m}{\\partial x_2} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Generally speaking, torch.autograd is an engine for computing vector-Jacobian product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebb24c4",
   "metadata": {},
   "source": [
    "## Computational Graph\n",
    "\n",
    "Conceptually, autograd keeps a record of data (tensors) & all executed operations (along with the resulting new tensors) in a directed acyclic graph (DAG) consisting of Function objects. In this DAG, leaves are the input tensors, roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.\n",
    "\n",
    "In a forward pass, autograd does two things simultaneously:\n",
    "\n",
    "- run the requested operation to compute a resulting tensor, and\n",
    "- maintain the operation’s gradient function in the DAG.\n",
    "\n",
    "The backward pass kicks off when .backward() is called on the DAG root. autograd then:\n",
    "\n",
    "- computes the gradients from each .grad_fn,\n",
    "- accumulates them in the respective tensor’s .grad attribute, and\n",
    "- using the chain rule, propagates all the way to the leaf tensors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c646b0a",
   "metadata": {},
   "source": [
    "## Exclude gradients from the graph\n",
    "\n",
    "torch.autograd tracks all operations on tensors with requires_grad=True. This is useful for training, but sometimes you want to exclude certain operations from the graph. For example, when you are evaluating a model and do not need to compute gradients, or tensors that don't require gradients, one can set the attribute requires_grad=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14261de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does `a` require gradients?: False\n",
      "Does `b` require gradients?: True\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(f\"Does `a` require gradients?: {a.requires_grad}\")\n",
    "# an output tensor requires gradients if at least one of its inputs requires gradients\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients?: {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbca0aef",
   "metadata": {},
   "source": [
    "In a NN, parameters that don’t compute gradients are usually called frozen parameters. It is useful to “freeze” part of your model if you know in advance that you won’t need the gradients of those parameters (this offers some performance benefits by reducing autograd computations).\n",
    "\n",
    "In finetuning, we freeze most of the model and typically only modify the classifier layers to make predictions on new labels. Let’s walk through a small example to demonstrate this. As before, we load a pretrained resnet18 model, and freeze all the parameters.\n",
    "\n",
    "Let's show this as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01f6da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# Freeze all the parameters in the network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1cc832",
   "metadata": {},
   "source": [
    "Let’s say we want to finetune the model on a new dataset with 10 labels. In resnet, the classifier is the last linear layer model.fc. We can simply replace it with a new linear layer (unfrozen by default) that acts as our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aaa4d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6844295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight False\n",
      "bn1.weight False\n",
      "bn1.bias False\n",
      "layer1.0.conv1.weight False\n",
      "layer1.0.bn1.weight False\n",
      "layer1.0.bn1.bias False\n",
      "layer1.0.conv2.weight False\n",
      "layer1.0.bn2.weight False\n",
      "layer1.0.bn2.bias False\n",
      "layer1.1.conv1.weight False\n",
      "layer1.1.bn1.weight False\n",
      "layer1.1.bn1.bias False\n",
      "layer1.1.conv2.weight False\n",
      "layer1.1.bn2.weight False\n",
      "layer1.1.bn2.bias False\n",
      "layer2.0.conv1.weight False\n",
      "layer2.0.bn1.weight False\n",
      "layer2.0.bn1.bias False\n",
      "layer2.0.conv2.weight False\n",
      "layer2.0.bn2.weight False\n",
      "layer2.0.bn2.bias False\n",
      "layer2.0.downsample.0.weight False\n",
      "layer2.0.downsample.1.weight False\n",
      "layer2.0.downsample.1.bias False\n",
      "layer2.1.conv1.weight False\n",
      "layer2.1.bn1.weight False\n",
      "layer2.1.bn1.bias False\n",
      "layer2.1.conv2.weight False\n",
      "layer2.1.bn2.weight False\n",
      "layer2.1.bn2.bias False\n",
      "layer3.0.conv1.weight False\n",
      "layer3.0.bn1.weight False\n",
      "layer3.0.bn1.bias False\n",
      "layer3.0.conv2.weight False\n",
      "layer3.0.bn2.weight False\n",
      "layer3.0.bn2.bias False\n",
      "layer3.0.downsample.0.weight False\n",
      "layer3.0.downsample.1.weight False\n",
      "layer3.0.downsample.1.bias False\n",
      "layer3.1.conv1.weight False\n",
      "layer3.1.bn1.weight False\n",
      "layer3.1.bn1.bias False\n",
      "layer3.1.conv2.weight False\n",
      "layer3.1.bn2.weight False\n",
      "layer3.1.bn2.bias False\n",
      "layer4.0.conv1.weight False\n",
      "layer4.0.bn1.weight False\n",
      "layer4.0.bn1.bias False\n",
      "layer4.0.conv2.weight False\n",
      "layer4.0.bn2.weight False\n",
      "layer4.0.bn2.bias False\n",
      "layer4.0.downsample.0.weight False\n",
      "layer4.0.downsample.1.weight False\n",
      "layer4.0.downsample.1.bias False\n",
      "layer4.1.conv1.weight False\n",
      "layer4.1.bn1.weight False\n",
      "layer4.1.bn1.bias False\n",
      "layer4.1.conv2.weight False\n",
      "layer4.1.bn2.weight False\n",
      "layer4.1.bn2.bias False\n",
      "fc.weight True\n",
      "fc.bias True\n"
     ]
    }
   ],
   "source": [
    "# show the layers of the model\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32020ff",
   "metadata": {},
   "source": [
    "As you can see, only the last layer's parameters are trainable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
